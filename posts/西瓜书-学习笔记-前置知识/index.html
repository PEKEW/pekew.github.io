<!DOCTYPE html>
<html
  lang="en"
  dir="ltr"
  
><meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">


<title>西瓜书 学习笔记 前置知识 | PeiKeWang&#39;s WebSite</title>

<meta name="generator" content="Hugo Eureka 0.9.3" />
<link rel="stylesheet" href="/css/eureka.min.9cec6350e37e534b0338fa9a085bf06855de3b0f2dcf857e792e5e97b07ea905d4d5513db554cbc26a9c3da622bae92d.css">
<script defer src="/js/eureka.min.fa9a6bf6d7a50bb635b4cca7d2ba5cf3dfb095ae3798773f1328f7950028b48c17d06276594e1b5f244a25a6c969a705.js"></script>













<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload"
  href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&amp;family=Noto&#43;Serif&#43;SC:wght@400;600;700&amp;display=swap"
  as="style" onload="this.onload=null;this.rel='stylesheet'">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/styles/base16/solarized-light.min.css"
   media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/highlight.min.js"
   crossorigin></script>
  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/languages/dart.min.js"
     crossorigin></script>
<link rel="stylesheet" href="/css/highlightjs.min.2958991528e43eb6fc9b8c4f2b8e052f79c4010718e1d1e888a777620e9ee63021c2c57ec7417a3108019bb8c41943e6.css" media="print" onload="this.media='all';this.onload=null">


<script defer type="text/javascript" src="/js/fontawesome.min.de149ff7f2ddc05cb4e1c473c0407f178365bb605dcfbe7d298e9d0164cc1894411633d430e3f8f0be1d5f02d4ca7497.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"
   integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ"  media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" 
  integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY"  crossorigin></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
   integrity="sha384-&#43;XBljXPPiv&#43;OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"  crossorigin></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false },
        { left: "\\(", right: "\\)", display: false },
        { left: "\\[", right: "\\]", display: true }
      ],
    });
  });
</script>


<script defer src="https://cdn.jsdelivr.net/npm/mermaid@8.14.0/dist/mermaid.min.js" 
  integrity="sha384-atOyb0FxAgN9LyAc6PEf9BjgwLISyansgdH8/VXQH8p2o5vfrRgmGIJ2Sg22L0A0"  crossorigin></script>


<link rel="icon" type="image/png" sizes="32x32" href="/images/icon_huea8837677a789ef35ad29240a1978919_1601044_32x32_fill_box_center_3.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/icon_huea8837677a789ef35ad29240a1978919_1601044_180x180_fill_box_center_3.png">

<meta name="description"
  content="确切来说，我是第一次“认真地”“从头到尾地”学习这本西瓜书。功利一些，这是数学所入学考试的参考书之一，但奈何没有重点谈起，所以只好从头到尾认真地学习。">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
      "@type": "ListItem",
      "position": 1 ,
      "name":"Posts",
      "item":"/posts/"},{
      "@type": "ListItem",
      "position": 2 ,
      "name":"西瓜书 学习笔记 前置知识",
      "item":"/posts/%E8%A5%BF%E7%93%9C%E4%B9%A6-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/"}]
}
</script>



<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "/posts/%E8%A5%BF%E7%93%9C%E4%B9%A6-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/"
    },
    "headline": "西瓜书 学习笔记 前置知识 | PeiKeWang\u0027s WebSite","datePublished": "2022-10-17T23:20:43+08:00",
    "dateModified": "2022-10-17T23:20:43+08:00",
    "wordCount":  5463 ,
    "author": {
        "@type": "Person",
        "name": ["PeiKeWang"]
    },
    "publisher": {
        "@type": "Person",
        "name": "WANG Chucheng",
        "logo": {
            "@type": "ImageObject",
            "url": "/images/icon.png"
        }
        },
    "description": "确切来说，我是第一次“认真地”“从头到尾地”学习这本西瓜书。功利一些，这是数学所入学考试的参考书之一，但奈何没有重点谈起，所以只好从头到尾认真地学习。"
}
</script><meta property="og:title" content="西瓜书 学习笔记 前置知识 | PeiKeWang&#39;s WebSite" />
<meta property="og:type" content="article" />


<meta property="og:image" content="/images/icon.png">


<meta property="og:url" content="/posts/%E8%A5%BF%E7%93%9C%E4%B9%A6-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/" />




<meta property="og:description" content="确切来说，我是第一次“认真地”“从头到尾地”学习这本西瓜书。功利一些，这是数学所入学考试的参考书之一，但奈何没有重点谈起，所以只好从头到尾认真地学习。" />




<meta property="og:locale" content="en" />




<meta property="og:site_name" content="PeiKeWang&#39;s WebSite" />






<meta property="article:published_time" content="2022-10-17T23:20:43&#43;08:00" />


<meta property="article:modified_time" content="2022-10-17T23:20:43&#43;08:00" />



<meta property="article:section" content="posts" />


<meta property="article:tag" content="Machine Learning" />









<meta property="og:see_also" content="/posts/%E8%A5%BF%E7%93%9C%E4%B9%A6-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" />



<meta property="og:see_also" content="/posts/%E8%A5%BF%E7%93%9C%E4%B9%A6-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%86%B3%E7%AD%96%E6%A0%91/" />



<meta property="og:see_also" content="/posts/%E8%A5%BF%E7%93%9C%E4%B9%A6-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" />








  <body class="flex min-h-screen flex-col">
    <header
      class="min-h-16 pl-scrollbar bg-secondary-bg fixed z-50 flex w-full items-center shadow-sm"
    >
      <div class="mx-auto w-full max-w-screen-xl"><script>
    let storageColorScheme = localStorage.getItem("lightDarkMode")
    if (((storageColorScheme == 'Auto' || storageColorScheme == null) && window.matchMedia("(prefers-color-scheme: dark)").matches) || storageColorScheme == "Dark") {
        document.getElementsByTagName('html')[0].classList.add('dark')
    }
</script>
<nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0">
    <a href="/" class="me-6 text-primary-text text-xl font-bold">PeiKeWang&#39;s WebSite</a>
    <button id="navbar-btn" class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
        <i class="fas fa-bars"></i>
    </button>

    <div id="target"
        class="hidden block md:flex md:grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20">
        <div class="md:flex md:h-16 text-sm md:grow pb-4 md:pb-0 border-b md:border-b-0">
            <a href="/#about" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">About</a>
            <a href="/posts/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  selected-menu-item  me-4">Posts</a>
            <a href="/docs/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">Docs</a>
        </div>

        <div class="flex">
            <div class="relative pt-4 md:pt-0">
                <div class="cursor-pointer hover:text-eureka" id="lightDarkMode">
                    <i class="fas fa-adjust"></i>
                </div>
                <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id="is-open">
                </div>
                <div class="absolute flex flex-col start-0 md:start-auto end-auto md:end-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40"
                    id='lightDarkOptions'>
                    <span class="px-4 py-1 hover:text-eureka" name="Light">Light</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Dark">Dark</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Auto">Auto</span>
                </div>
            </div>
        </div>
    </div>

    <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id="is-open-mobile">
    </div>

</nav>
<script>
    
    let element = document.getElementById('lightDarkMode')
    if (storageColorScheme == null || storageColorScheme == 'Auto') {
        document.addEventListener('DOMContentLoaded', () => {
            window.matchMedia("(prefers-color-scheme: dark)").addEventListener('change', switchDarkMode)
        })
    } else if (storageColorScheme == "Light") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'sun')
        element.firstElementChild.classList.add('fa-sun')
    } else if (storageColorScheme == "Dark") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'moon')
        element.firstElementChild.classList.add('fa-moon')
    }

    document.addEventListener('DOMContentLoaded', () => {
        getcolorscheme();
        switchBurger();
    });
</script>
</div>
    </header>
    <main class="grow pt-16">
        <div class="pl-scrollbar">
          <div class="mx-auto w-full max-w-screen-xl lg:px-4 xl:px-8">
  
  
  <div class="grid grid-cols-2 gap-4 lg:grid-cols-8 lg:pt-12">
    <div
      class=" bg-secondary-bg col-span-2 rounded px-6 py-8 lg:col-span-6"
    >
      <article class="prose">
  <h1 class="mb-4">西瓜书 学习笔记 前置知识</h1>

  <div
  class="text-tertiary-text not-prose mt-2 flex flex-row flex-wrap items-center"
>
  <div class="me-6 my-2">
    <i class="fas fa-calendar me-1"></i>
    <span
      >2022-10-17</span
    >
  </div>
  <div class="me-6 my-2">
    <i class="fas fa-clock me-1"></i>
    <span>11 min read</span>
  </div>

  
    <div class="me-6 my-2">
      <i class="fas fa-folder me-1"></i>
      
        <a href="/categories/ai/" class="hover:text-eureka"
          >AI</a
        >
      
    </div>
  

  
    <div class="me-6 my-2">
      <i class="fas fa-th-list me-1"></i>
      
        <a href="/series/zhihuazhou-machinelearning-reading/" class="hover:text-eureka"
          >ZhiHuaZhou-MachineLearning-Reading</a
        >
      
    </div>
  
</div>


  
  

  <blockquote>
<p>确切来说，我是第一次“认真地”“从头到尾地”学习这本西瓜书。功利一些，这是数学所入学考试的参考书之一，但奈何没有重点谈起，所以只好从头到尾认真地学习。由于时间紧张，也许日后没有读第二遍的机会，遂写此笔记。每读一章，记下重点内容和公式推导。希望能够借此方法高效学习一次，也顺便做出一些微小的产出。希望我考博顺利。希望我圆梦。</p>
</blockquote>
<h1 id="绪论">绪论</h1>
<h2 id="引言">引言</h2>
<p>引言部分虽然没有一些比较硬的知识，但有些概念还是十分有必要提到的。这些概念某种程度上来说对机器学习乃至于深度学习的算法的抽象理解有些许帮助。</p>
<h3 id="11">1.1</h3>
<ul>
<li>机器学习，是一门通过计算的手段、利用经验来改善系统自身性能的学科。从这句话中能够理解到“学习”的意思。</li>
<li>机器学习研究的内容，就是如何在计算机上，从数据中产生“模型”的算法。</li>
<li>模型泛指从数据中学习到的结果。</li>
<li>模式指的是局部性的结果。例如，决策树是一个模型，而一条规则是一个模式。</li>
</ul>
<h3 id="12">1.2</h3>
<ul>
<li>从数据中学习模型的过程被叫做学习或训练。这个过程中用到的数据叫做训练集。</li>
<li>训练好的模型对应着数据中潜在的某种规律，叫做假设。假设是一个比较重要的隐含概念，虽然后面不常提及，但其经常作为“默认条件”。</li>
<li>预测任务分类“分类”任务和“回归任务”，从字面意思上理解，分类任务是对输入数据类别的划分，而回归任务是对输入数据某个隐含特征的拟合。 简单来说，分类任务是离散的预测任务，如“好瓜”和“坏瓜”。而拟合任务是连续的预测任务，如“西瓜的成熟度”。</li>
<li>一般地，预测任务是希望通过对训练集${(x_1,y_1),\dots,(x_m,y_m)}$进行学习，建立一个从输入空间$\mathcal{X}$到输出空间$\mathcal{Y}$的映射$f: \mathcal{X} \mapsto \mathcal{Y}$。而分类任务和回归任务的区别在于输出空间的取值范围。</li>
<li>学习任务分为“监督学习”和“无监督学习”，当然目前也有一类重点问题叫做“半监督学习”。监督学习和无监督学习的区分在于是否有标记信息，如，通过已标记信息的数据集对西瓜和南瓜进行分类，数据中已经给出了某些特征已经这些特征对应的是南瓜还是西瓜。这种分类任务被称为监督学习，而不需要数据中给出某些特征对应的某种标签，对数据集进行聚类。这种聚类任务被称为无监督学习。半监督学习顾名思义，是监督学习和无监督学习的一种结合，使用大量的未标记数据，同时也使用人工标记的数据进行一些特定任务。</li>
<li>模型训练中有一个很重要的能力———泛化能力，即就是模型适用于新样本的能力。因为在训练模型时，我们不可能使用样本空间中所有的样本进行训练，通常只能采样到很小的一部分子空间进行模型训练，但我们仍希望在小尺寸的空间中学习到的模型能够适应于整个样本空间，特别是那些没有遇到过的新样本。换言之，我们希望在子空间中学习到的模型能够很好的反应整个样本空间的某些特征，即为模型的泛化能力。</li>
<li>通常，我们需要假设样本空间中的样本服从“i.i.d”。</li>
</ul>
<h3 id="14">1.4</h3>
<ul>
<li>机器学习算法在学习过程中对某种类型的假设的偏好，被称为“归纳偏好”。 一个有效的机器学习算法是必须有归纳偏好的，否则没有意义可言。归纳偏好可以看做是算法在假设空间中学习到的启发式规则。直观理解为，神经网络中，当前神经元的输入权重有大有小，表示着当前神经元对其输入的响应偏好。有些神经元必须有较大的输出，才能激活当前神经元，这可以被认为是一种归纳偏好。</li>
<li>可以使用奥卡姆剃刀原则引导算法建立争取的偏好：“如果有多个假设和观察一致，取最简单的那个”，”如无必要，勿增实体“</li>
<li>算法的归纳偏好实际上是关于“什么样的模型/算法更好”的假设。但这样的假设是不能脱离具体问题的，一旦脱离了具体问题，“什么样的算法更好将不再有意义”。</li>
</ul>
<p>简单论述如下：</p>
<p>简单起见，做如下假设和规定：样本空间$\mathcal{X}$ 和假设空间$\mathcal{H}$都是离散的。令$P(h|X,\mathfrak{L}_a)$代表$\mathfrak{L}_a$算法基于训练样本$X$产生假设$h$的概率。令$f$代表我们希望学到的目标函数，以训练集外误差作为性能度量，考虑二分类问题，且$f$均匀分布。则对于所有可能得$f$，误差和为:</p>
<div>
$$\sum_f E_{ode}(\mathfrak{L}_a|X,f) = \sum_f\sum_h\sum_{x\in\mathcal{X}-X}P(x)\mathcal{I}(h(x)\not=f(x))P(h|,X,\mathfrak{L}_a)$$
</div>
<p>$$ =\sum_{x\in\mathcal{X}-X}P(x)\sum_hP(h|X,\mathfrak{L}_a)\sum_f\mathcal{I}(h(x)\not=f(x))$$</p>
<p>目标空间每有一个样本，就需要有两种目标函数的可能，把这个样本映射到${0, 1}$,因此，原式
$$ =\sum_{x\in\mathcal{X}-X}P(x)\sum_hP(h|X,\mathfrak{L}_a)\frac12 2^{|\mathcal{X}|}$$</p>
<p>$$ =\frac12 2^{|\mathcal{X}|}\sum_{x\in\mathcal{X}-X}P(x)\sum_hP(h|X,\mathfrak{L}_a)$$</p>
<p>$$ =\frac12 2^{|\mathcal{X}|}\sum_{x\in\mathcal{X}-X}P(x) \cdot 1$$</p>
<p>因此，对于一个并不具体的任务（任务不具体，则任何$f$都可能存在）来说，总误差是和学习算法无关的。（NFL定理的简单论述）</p>
<h2 id="模型评估与选择">模型评估与选择</h2>
<h3 id="21">2.1</h3>
<ul>
<li>错误率是只分类错误的样本数占总数的比例</li>
<li>精度则为1-错误率</li>
<li>模型在实际预测输出与样本的标签之间的差异叫做”误差“</li>
<li>泛化误差指的是模型在新样本（非训练集）上的误差。</li>
<li>我们的目的是模型的泛化误差尽可能小，为了达到这个目的，应该尽量让模型学习出适用于所有样本的潜在的普遍规律。</li>
<li>在训练模型时，容易出现两种现象，”过拟合“和”欠拟合“。</li>
<li>过拟合指的是模型对于训练样本学的太好了，把关于训练样本的特殊规律当做了所有样本的普遍规律。</li>
<li>欠拟合和过拟合相反，指的是模型没有学习的很好</li>
<li>欠拟合比较容易解决，比如增加神经网络的训练轮数，增加决策树的分支等。</li>
<li>过拟合无法彻底避免，所有针对过拟合的措施仅仅能够做到缓解过拟合。</li>
<li>过拟合问题是机器学习面临的关键障碍。</li>
<li>过拟合问题无法解决本质上是因为我们相信&quot;$P\not=NP$&quot;</li>
</ul>
<h3 id="22">2.2</h3>
<ul>
<li>当我们需要评估一个模型的”学习质量“时，需要引入一个测试集来测试模型在测试样本的误差（新样本）</li>
<li>一般我们从数据集中划分训练集和测试集，训练集和测试集的划分应该尽量互斥，且均匀分布。互斥的原因是训练集测量的是模型的泛化误差，而均匀分布是我们希望在划分数据时尽可能的避免引入额外误差。</li>
<li>没有哪个评估方法一定优于别的方法。</li>
</ul>
<h4 id="留出法">留出法</h4>
<ul>
<li>留出法直接将数据集$D$划分为两个互斥的集合$S$和$T$,且$D=S\cup T， S\cap T= \varnothing$</li>
<li>使用留出法时，为了避免数据分布的不一致，可以进行分层采样，即对于每一类的标签，我们在测试集和训练集中保留的比例应该尽可能一致。</li>
<li>使用留出法时，一般采用若干次随机的划分，重复进行实验评估后取平均值作为留出法的评估结果。</li>
</ul>
<h4 id="交叉验证">交叉验证</h4>
<ul>
<li>交叉验证法首先把数据集划分为$k$个互斥的子集，然后分别把每个子集当做验证集，其余子集当做训练集。一共需要进行$k$次独立的训练验证，并取均值，一般$k$采用5，10，20.</li>
<li>和留出法类似，交叉验证也通常重复$p$次取均值，被称为$p$次$k$折交叉验证。</li>
<li>当交叉验证中的每个子集只有一个样本时，这种交叉验证方法被称为留一法。留一法的好处是，使用训练集只比原始的数据集少一个样本，这样能够使得训练集的误差期望和实际的误差期望相似。但当面对数据集较大的情况时，留一法的效率在计算上不可行。</li>
</ul>
<h4 id="自助法">自助法</h4>
<ul>
<li>自助法即为自助采样法，具体过程为，通过反复地对训练集进行又放回采样，生成训练集$D&rsquo;$，我们认为在通过反复地采样，总的数据集中最终会有$D\backslash D&rsquo;$个样本始终没有被采样到，取这样没有采样到的样本作为验证集。</li>
<li>自助法的原理基于重要极限，做简单估计：样本在$m$此采样中始终不被采样到的概率$(1-\frac1m)^m$的极限为:
$$\text{lim}_{m\to\inf}(1-\frac1m)^m \to \frac1e \approx 0.368$$</li>
<li>自助法适合于数据集小，难以有效划分的数据集，但自助法产生的数据改变了数据的分布，可能引入新的偏差。</li>
<li>除测试集，训练集外，验证集是主要是针对模型的选择和调参的。</li>
</ul>
<h3 id="23">2.3</h3>
<ul>
<li>性能度量指的是衡量模型泛化能力的实验估计方法。</li>
<li>在预测任务中，均方误差是最常用的性能度量，其格式如下
$$E(f;\mathcal{D}) = \int_{x\sim \mathcal{D}} (f(x) - y)^2p(x)dx$$</li>
<li>其中，$\mathcal{D}$是数据分布，$p(\cdot)$是概率密度函数, 具体的，在实验中我们常用其离散形式：
$$E(f;\mathcal{D}) = \frac1m \sum^{m}_{i=1}(f(x_i) - y_i)^2$$</li>
<li>分类任务中的性能度量常用错误率和精度，具体定义为:
$$E(f;\mathcal{D}) = \frac1m \sum^m_{i=1}\mathcal{I}(f(x_i)\not=y_i)$$
$$acc(f;\mathcal{D}) = 1 - E(f;D)$$</li>
<li>查准率、查全率和$F1$是针对某些特定任务需求的指标, 简单来说，查准率（precision）用来衡量预测正例的准确性，查全率(recall) 用来衡量预测为正的全面性（查准率和查全率也常被叫做精确率和召回率）
<ul>
<li>根据字面意思，查准率是和预测正例准确性相关的，预测为正例有两种情况：预测正，正确和预测正，错误。</li>
<li>而查全率表示预测为正例的全面性，隐含着要考察所有正例的样本有多少预测正确。</li>
</ul>
</li>
<li>要给出查准率和查全率的精确定义，我们先定义分类结果的混淆矩阵（二分类）</li>
</ul>
<table style="text-align: center;">
   <tr>
      <td rowspan="2">真实情况</td>
      <td colspan="2">预测结果</td>
   </tr>
   <tr>
      <td>真</td>
      <td>假</td>
   </tr>
   <tr>
      <td>真</td>
      <td>TP（真正例）</td>
      <td>FN（假反例）</td>
   </tr>
   <tr>
      <td>假</td>
      <td>FP（假正例）</td>
      <td>TN（真反例）</td>
   </tr>
</table>
<ul>
<li>查准率$P$和查全率$R$的定义为:
$$P = \frac{TP}{TP+FP}$$
$$R = \frac{TP}{TP+FN}$$</li>
<li>这种公式无论是从哪种角度来看都比较绕口，所以死记硬背绝对不是正确的方法，要按照他们的度量角度理解和记忆，例如，查准率度量的是预测为正的准确性，那么公式里肯定质保函$*P$,即就是，预测为正，不管正确与否。</li>
<li>查准率和查全率是相互矛盾的度量。</li>
<li>当我们使用查准率和查全率作为标准挑选模型时，可以画出P-R曲线图，具体操作为：
<ul>
<li>首先对每一个模型输出样本的预测结果倒序排序（假设输出空间为[0,1]的连续值，越接近1表示越有可能是正例)</li>
<li>遍历到一个样本时，以当前样本预测的置信度为分类阈值, 依次计算每个样本对应的查全率和查准率。</li>
<li>以查准率为纵坐标，查全率为横坐标作图，得到P-R曲线图。</li>
<li>P-R曲线图图例见西瓜书31页。</li>
</ul>
</li>
<li>当某个模型对应的P-R曲线图完全“包住”另一个模型的P-R曲线图时，我们认为前者的性能更优。</li>
<li>大多数情况下，很难有“完全包住”的情况，这时如果需要判断模型的优劣关系时，可以计算P-R去线下面积。</li>
<li>P-R曲线下面积一定程度上表整了模型在查准率和查全率都高的比例。</li>
<li>P-R曲线下面积难以计算，因此有进入“平衡点的概念”，平衡点指的是查准率和查全率相等时的点，我们认为平衡点大的模型更优。</li>
<li>$F1$是查准率和查全率倒数的调和平均，定义为：
$$\frac1F = \frac12(\frac1P+\frac1R)$$</li>
<li>$F1$的一般形式为$F_\beta$</li>
<li>$F_\beta$是带有偏好行为的$F1$，其偏好行为由$\beta$定义，具体为：
$$F_\beta = \frac{1}{1+\beta^2}(\frac1P + \frac{\beta^2}R)$$</li>
<li>当目标任务是多分类，或多次二分类时，我们可以综合考察查准率和查全率，有两种具体的方法：</li>
<li>宏查准率，宏查全率：计算每个混淆矩阵的查全率和查准率，取平均。</li>
<li>微查准率，微查全率：对混淆矩阵取平均，然后计算查准率和查全率。</li>
<li>ROC曲线是从不同任务下的泛化性能好坏来度量模型的工具。</li>
<li>ROC引入了两个新的指标，基于混淆矩阵，有:
$$TPR = \frac{TP}{TP+FN}$$
$$FPR = \frac{FP}{TN+FP}$$</li>
<li>分别为真正例率（TPR）和假正例率（FPR），同样，这种指标死记硬背永远也记不住，比如理解：真正例率衡量的是“正确预测为正的占所有正例的比例”，而假正例率为错误预测为正的，占所有负例的比例</li>
<li>ROC曲线的绘制和P-R曲线绘制方法类似</li>
<li>判断模型优劣仍然可以考察“完全包住”和ROC下曲线面积，即AUC</li>
<li>AUC的估算类似于梯形公式
$$AUC = \frac12 \sum^{m-1}<em>{i=1}(x</em>{i+1}-x_i)(y_i+y_{i+1})$$</li>
<li>在实际的分类任务中，我们不能默认不同的分类错误会有相同的代价，例如，将阴性肿瘤判断为阳性和将阳性肿瘤预测为阴性是完全不同的代价，因此，我们对于这些分类错误赋予非均等代价</li>
<li>在非均等代价下，ROC不能反应模型的期望总代价，因此借助代价曲线衡量，代价曲线的绘制方法为：
<ul>
<li>横轴为正例概率代价 $P(+)cost = \frac{p\times cost_{01}}{p\times cost_{01} +  (1-p)\times cost_{10}}$</li>
<li>纵轴为归一化代价 $cost_{norm} = \frac{FNR\times p \times cost_{01} + FPR \times (1-p) \times cost_{10}}{p\times cost_{01} + (1-p)\times cost(10)}$</li>
<li>其中$p$是分类为正例的概率，FNR是假反例率,$FNR = 1- TPR$</li>
<li>代价曲线图图例见西瓜书37页</li>
</ul>
</li>
</ul>
<h3 id="24">2.4</h3>
<ul>
<li>假设检验，假设是对模型的泛化误差分布的判断或者猜想，检验实际结果是否接受此猜想</li>
</ul>
<h4 id="二项检验">二项检验</h4>
<p>二项检验即二项分布参数$p$的检验，设$m$为样本个数，$\epsilon$为模型的泛化错误率，$\alpha$是显著度。
则在$1-\alpha$的概率内能观察到的最大错误率为</p>
<div>
$$\bar\epsilon = \min \epsilon\quad\quad s.t. \sum^m_{i=\epsilon\times m+1}\begin{pmatrix}m\\i\end{pmatrix}\epsilon^i(1-\epsilon^{m-i})<\alpha$$
</div>
<p>这个公式和西瓜书(2016年1月第1版)中不一致，应该是书中错误，本质上就是概率论知识，推导过程略。</p>
<ul>
<li>当我们要做多次估计，例如多次重复留出法或者多次重复交叉验证时，可以使用t检验.</li>
<li>t检验和二次检验都是针对单个模型的泛化性能的假设进行验证的，很多时候我们需要对多个模型进行假设检验以进行性能比较，适用于此类情形的方法有：
<ul>
<li>交叉验证t检验，前提是测试错误率均为泛化错误率的独立采样</li>
<li>McNemar检验</li>
<li>Friedman检验</li>
<li>Nemenyi后续检验</li>
</ul>
</li>
</ul>
<h3 id="25">2.5</h3>
<ul>
<li>偏差和方差是引起泛化误差的原因，“偏差-方差分解”是解释模型泛化性能的一种重要工具。</li>
<li>在进行偏差-方差分解前，需要明确：假设$f(x;D)$是算法在数据集$D$上学到的模型$f$的预测输出，由于数据集的不同划分是对学到的模型有影响的，换句话说，训练集的数据分布影响学到的模型，也影响了模型的预测输出，因此，$f(x;D)$可以理解为随机变量。</li>
<li>以回归任务为例，算法学到的模型的期望预测值为$\bar f(x) = \mathbb{E}_D[f(x;D)]$，（如果没有上一条前提，我们很难理解一个“算法学到的模型”的期望是什么东西。</li>
<li>有了预测期望，我们就可以讨论当数据集划分不同时（样本数量相同）产生的方差为$var(x) = \mathbb{E}_D\left[(f(x;D) - \bar f(x))^2\right]$</li>
<li>噪声指的是标记标签和真实标签的差别，类似也可以用随机变量表示：$\varepsilon^2  = \mathbb{E}_D[(y_D-y)^2]$,为方便后续讨论，我们认为此项为0。</li>
<li>期望输出和真实标记的差别为偏差:$bias^2(x) = (\bar f(x) - y)^2$</li>
<li>由此对泛化误差进行偏差-方差分解</li>
</ul>
<p>$$E(f;D) = bias^2(x) + var(x) + \varepsilon^2$$</p>
<p>由于公式太多不想打字, 故推导过程略。</p>
<ul>
<li>偏差和方差是冲突的，训练不足时，模型的拟合能力不足，所以不同数据集的扰动对模型有较大影响，因此，这时候偏差主导泛化错误率。</li>
<li>当模型的拟合能力逐渐增强，这时方差主导泛化误差。</li>
<li>以上便是偏差-方差窘境。</li>
</ul>

</article>


      
        <div class="my-4">
    
    <a href="/tags/machine-learning/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 me-2 hover:text-eureka">#Machine Learning</a>
    
</div>
      

      



      
        <div class="py-2">
  
    <div class="my-8 flex flex-col items-center md:flex-row">
      <a href="/authors/peikewang/" class="md:me-4 text-primary-text h-24 w-24">
        
        
          <img
            src="/images/icon.png"
            class="bg-primary-bg w-full rounded-full"
            alt="Avatar"
          />
        
      </a>
      <div class="mt-4 w-full md:mt-0 md:w-auto">
        <a
          href="/authors/peikewang/"
          class="mb-2 block border-b pb-1 text-lg font-bold"
        >
          <h3>PeiKe Wang</h3>
        </a>
        <span class="block pb-2">artificial intelligence | machine learning | deep learning | optimization theory.</span>
        
          
          
          
          
          <a href="mailto:pekewang@hotmail.com" class="me-2">
            <i class="fas fa-envelope"></i>
          </a>
        
          
          
          
          
          <a href="https://github.com/PEKEW" class="me-2">
            <i class="fab fa-github"></i>
          </a>
        
          
          
          
          
          <a href="https://weibo.com/u/6981005635" class="me-2">
            <i class="fab fa-weibo"></i>
          </a>
        
      </div>
    </div>
  
</div>

      

      
  <div
    class="-mx-2 mt-4 flex flex-col border-t px-2 pt-4 md:flex-row md:justify-between"
  >
    <div>
      
        <span class="text-primary-text block font-bold"
          >Previous</span
        >
        <a href="/posts/%E8%A5%BF%E7%93%9C%E4%B9%A6-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" class="block">西瓜书 学习笔记 线性模型</a>
      
    </div>
    <div class="mt-4 md:mt-0 md:text-right">
      
        <span class="text-primary-text block font-bold">Next</span>
        <a href="/posts/%E6%8A%8A%E6%89%80%E6%9C%89typing-work%E4%BC%98%E9%9B%85%E5%9C%B0%E8%BF%81%E7%A7%BB%E8%87%B3vim-vim%E5%AE%8C%E5%85%A8%E9%85%8D%E7%BD%AE/" class="block">把所有typing Work优雅地迁移至vim VIM完全配置</a>
      
    </div>
  </div>


      



    </div>
    
      <div class="col-span-2">
        
          
<div class="bg-secondary-bg prose max-w-none rounded p-6">
  <h3>Series of Posts</h3>
  
    
      <a href="/posts/%E8%A5%BF%E7%93%9C%E4%B9%A6-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="no-underline">西瓜书 学习笔记 神经网络</a>
      <br />
    
      <a href="/posts/%E8%A5%BF%E7%93%9C%E4%B9%A6-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%86%B3%E7%AD%96%E6%A0%91/" class="no-underline">西瓜书 学习笔记 决策树</a>
      <br />
    
      <a href="/posts/%E8%A5%BF%E7%93%9C%E4%B9%A6-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" class="no-underline">西瓜书 学习笔记 线性模型</a>
      <br />
    
      <a href="/posts/%E8%A5%BF%E7%93%9C%E4%B9%A6-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/" class="no-underline">西瓜书 学习笔记 前置知识</a>
      <br />
    
  
</div>

        
        
      </div>
    

    
    
  </div>

  
    <script>
      document.addEventListener("DOMContentLoaded", () => {
        hljs.highlightAll();
      });
    </script>

          </div>
        </div>
      
    </main>
    <footer class="pl-scrollbar">
      <div class="mx-auto w-full max-w-screen-xl"><div class="text-center p-6 pin-b">
    <p class="text-sm text-tertiary-text">&copy; 2022</a>
  </p>
</div></div>
    </footer>
  </body>
</html>
